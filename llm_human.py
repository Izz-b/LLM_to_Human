# -*- coding: utf-8 -*-
"""LLM_Human.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-FbYCTUqHJMrPzx1gQySSbOgqhFqnqk
"""

!pip show transformers

!pip install --upgrade transformers

import transformers
print(transformers.__version__)

!pip install transformers datasets

# Text Humanizer - Transformer du texte LLM en texte plus humain
# Notebook Google Colab optimis√©

# === 1. Installation et configuration ===
# Installation silencieuse des d√©pendances n√©cessaires
!pip install -q fastapi uvicorn transformers datasets tqdm ipywidgets pyngrok

# Import des biblioth√®ques n√©cessaires
import os
import torch
import numpy as np
import pandas as pd
import re
import json
import time
import threading
from typing import List, Dict, Optional, Union
from datetime import datetime
from tqdm.notebook import tqdm
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from google.colab import output, drive
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from functools import lru_cache
import transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

# V√©rification de la disponibilit√© du GPU et affichage des informations
def check_gpu():
    """V√©rifie la disponibilit√© du GPU et affiche les informations"""
    if torch.cuda.is_available():
        print(f"üéâ GPU disponible: {torch.cuda.get_device_name(0)}")
        # Afficher la m√©moire GPU disponible
        print(f"M√©moire GPU totale: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        !nvidia-smi
        return True
    else:
        print("‚ö†Ô∏è Pas de GPU d√©tect√©. L'entra√Ænement sera plus lent sur CPU.")
        return False

# Cr√©ation de la structure de r√©pertoires avec gestion des erreurs
def setup_directories():
    """Cr√©e la structure de r√©pertoires n√©cessaire"""
    directories = ["model/saved_model", "data", "exports"]
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception as e:
            print(f"Erreur lors de la cr√©ation du r√©pertoire {directory}: {e}")
    print("‚úÖ Structure de r√©pertoires cr√©√©e")

# === 2. D√©finition du mod√®le d'humanisation ===
class TextHumanizerModel:
    """Mod√®le pour transformer du texte LLM en texte plus humain"""

    def __init__(self, model_name: str = "t5-small", device: Optional[str] = None, max_length: int = 512):
        """
        Initialise le mod√®le de r√©-humanisation.

        Args:
            model_name: Nom ou chemin du mod√®le Hugging Face
            device: Appareil sur lequel ex√©cuter le mod√®le ('cpu' ou 'cuda')
            max_length: Longueur maximale des entr√©es/sorties
        """
        self.model_name = model_name
        self.max_length = max_length
        self.cache = {}  # Cache simple pour les r√©sultats fr√©quents

        # D√©terminer le dispositif (GPU par d√©faut dans Colab si disponible)
        if device:
            self.device = device
        else:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Chargement du mod√®le avec gestion des erreurs
        try:
            print(f"Chargement du mod√®le {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

            # Optimisations pour Google Colab
            if self.device == "cuda":
                # Utiliser la pr√©cision mixte pour √©conomiser de la m√©moire
                self.model = self.model.half()  # Conversion en FP16

            self.model.to(self.device)
            print(f"‚úÖ Mod√®le charg√© sur {self.device}")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le {model_name}: {str(e)}")
            raise

    def save_model(self, output_dir: str):
        """
        Sauvegarde le mod√®le et le tokenizer avec m√©tadonn√©es.

        Args:
            output_dir: Chemin du r√©pertoire o√π sauvegarder le mod√®le
        """
        try:
            os.makedirs(output_dir, exist_ok=True)

            # Sauvegarde avec barre de progression
            print("Sauvegarde du mod√®le...")
            self.model.save_pretrained(output_dir)
            self.tokenizer.save_pretrained(output_dir)

            # Sauvegarde des m√©tadonn√©es avec timestamp
            metadata = {
                "model_name": self.model_name,
                "device": self.device,
                "max_length": self.max_length,
                "saved_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            with open(os.path.join(output_dir, "model_info.json"), "w") as f:
                json.dump(metadata, f, indent=2)

            print(f"‚úÖ Mod√®le sauvegard√© dans {output_dir}")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde du mod√®le: {str(e)}")
            return False

    @classmethod
    def load_model(cls, model_path: str, device: Optional[str] = None):
        """
        Charge un mod√®le sauvegard√© avec gestion des erreurs.

        Args:
            model_path: Chemin vers le mod√®le sauvegard√©
            device: Appareil sur lequel charger le mod√®le

        Returns:
            TextHumanizerModel: Instance du mod√®le charg√©
        """
        try:
            # Cr√©er une instance vide
            instance = cls.__new__(cls)

            # Initialiser manuellement les attributs n√©cessaires
            instance.model_path = model_path
            instance.cache = {}

            # Configurer le dispositif
            if device:
                instance.device = device
            else:
                instance.device = "cuda" if torch.cuda.is_available() else "cpu"

            # Charger les m√©tadonn√©es
            metadata = {}
            info_path_json = os.path.join(model_path, "model_info.json")
            info_path_txt = os.path.join(model_path, "model_info.txt")

            if os.path.exists(info_path_json):
                with open(info_path_json, "r") as f:
                    metadata = json.load(f)
                instance.model_name = metadata.get("model_name", "custom_model")
                instance.max_length = metadata.get("max_length", 512)
            elif os.path.exists(info_path_txt):
                with open(info_path_txt, "r") as f:
                    for line in f:
                        key, value = line.strip().split(": ", 1)
                        metadata[key] = value
                instance.model_name = metadata.get("model_name", "custom_model")
                instance.max_length = 512
            else:
                instance.model_name = "custom_model"
                instance.max_length = 512

            # Charger le tokenizer et le mod√®le avec barre de progression
            print(f"Chargement du mod√®le depuis {model_path}...")
            instance.tokenizer = AutoTokenizer.from_pretrained(model_path)
            instance.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

            # Optimisations pour Google Colab
            if instance.device == "cuda":
                # Utiliser la pr√©cision mixte pour √©conomiser de la m√©moire
                instance.model = instance.model.half()

            instance.model.to(instance.device)
            print(f"‚úÖ Mod√®le charg√© sur {instance.device}")

            return instance
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
            raise

    def tokenize(self, text: Union[str, List[str]]):
        """
        Tokenise le texte d'entr√©e avec gestion de la longueur maximale.

        Args:
            text: Texte ou liste de textes √† tokeniser

        Returns:
            Tokens encod√©s
        """
        return self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

    @lru_cache(maxsize=128)  # Cache pour les requ√™tes fr√©quentes
    def _generate_cached(self, input_text: str, **kwargs):
        """Version mise en cache de la g√©n√©ration pour les textes fr√©quents"""
        # Tokeniser
        inputs = self.tokenize([input_text])

        # G√©n√©rer avec contr√¥le d'erreurs
        try:
            with torch.no_grad():
                output_ids = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **kwargs
                )

            # D√©coder
            return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration pour '{input_text[:30]}...': {str(e)}")
            return input_text  # Retourner le texte original en cas d'erreur

    def generate(self, input_text: Union[str, List[str]], **kwargs):
        """
        G√©n√®re du texte humanis√© √† partir du texte d'entr√©e avec optimisations.

        Args:
            input_text: Texte ou liste de textes √† humaniser
            **kwargs: Arguments suppl√©mentaires pour la g√©n√©ration

        Returns:
            List[str]: Liste des textes humanis√©s
        """
        # Pr√©parer les entr√©es
        single_input = isinstance(input_text, str)
        if single_input:
            input_text = [input_text]

        # Pr√©fixer les entr√©es pour indiquer la t√¢che
        prefixed_inputs = [f"humanize: {text}" for text in input_text]

        # Param√®tres par d√©faut pour la g√©n√©ration
        generation_kwargs = {
            "max_length": min(self.max_length, 512),
            "num_beams": 4,
            "length_penalty": 1.0,
            "early_stopping": True,
            "no_repeat_ngram_size": 3,
            "temperature": 0.8,
            "top_k": 50,
            "top_p": 0.95,
            "do_sample": True
        }

        # Mettre √† jour avec les kwargs fournis
        generation_kwargs.update(kwargs)

        # G√©n√©rer les r√©ponses
        results = []

        for text in prefixed_inputs:
            # Utiliser la version mise en cache pour les textes individuels
            result = self._generate_cached(text, **generation_kwargs)
            results.append(result)

        # Retourner un seul r√©sultat si l'entr√©e √©tait une cha√Æne
        if single_input:
            return results[0]
        return results

# === 3. D√©finition du dataset pour l'entra√Ænement ===
class HumanizerDataset(Dataset):
    """Dataset pour l'entra√Ænement du mod√®le de r√©-humanisation."""

    def __init__(self, texts, human_texts, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.texts = texts
        self.human_texts = human_texts
        self.max_length = max_length

        # Pr√©fixer les entr√©es pour indiquer la t√¢che
        self.input_texts = [f"humanize: {text}" for text in texts]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Tokeniser l'entr√©e avec gestion d'erreurs
        try:
            # Tokeniser l'entr√©e
            input_encoding = self.tokenizer(
                self.input_texts[idx],
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )

            # Tokeniser la sortie
            with self.tokenizer.as_target_tokenizer():
                target_encoding = self.tokenizer(
                    self.human_texts[idx],
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt"
                )

            # Pr√©parer les entr√©es pour l'entra√Ænement
            input_ids = input_encoding["input_ids"].squeeze()
            attention_mask = input_encoding["attention_mask"].squeeze()
            labels = target_encoding["input_ids"].squeeze()

            # Remplacer les tokens de padding par -100 pour les ignorer
            labels[labels == self.tokenizer.pad_token_id] = -100

            return {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": labels
            }
        except Exception as e:
            # En cas d'erreur, retourner un exemple vide (sera ignor√©)
            print(f"‚ùå Erreur lors de la tokenisation de l'exemple {idx}: {str(e)}")
            dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)
            return {
                "input_ids": dummy_tensor,
                "attention_mask": dummy_tensor,
                "labels": dummy_tensor
            }
def evaluate_model(model, test_data=None, num_samples=10):
    """
    √âvalue le mod√®le et affiche les m√©triques pertinentes

    Args:
        model: Le mod√®le TextHumanizerModel √† √©valuer
        test_data: DataFrame contenant des paires de textes pour l'√©valuation
        num_samples: Nombre d'exemples √† afficher
    """
    print("üìä √âVALUATION DU MOD√àLE")
    print("=" * 50)

    # Si aucune donn√©e de test n'est fournie, utiliser quelques exemples standards
    if test_data is None:
        test_data = pd.DataFrame({
            'llm_text': [
                "Les r√©sultats de l'analyse indiquent une corr√©lation positive entre les variables A et B avec un coefficient de 0.78.",
                "La proc√©dure d'installation requiert les √©tapes suivantes: t√©l√©chargement du fichier, extraction des donn√©es et configuration des param√®tres initiaux.",
                "L'√©tude d√©montre que la consommation r√©guli√®re de fruits et l√©gumes est associ√©e √† une r√©duction de 23% du risque de maladies cardiovasculaires."
            ],
            'human_text': [
                "Notre analyse montre que A et B sont fortement li√©s - on a trouv√© un coefficient de 0.78!",
                "Pour installer le programme, vous devez d'abord t√©l√©charger le fichier, puis extraire les donn√©es et enfin configurer vos param√®tres de d√©part.",
                "D'apr√®s l'√©tude, manger des fruits et l√©gumes r√©guli√®rement pourrait r√©duire votre risque de probl√®mes cardiaques d'environ 23%."
            ]
        })

    # G√©n√©rer des pr√©dictions
    llm_texts = test_data['llm_text'].tolist()
    human_texts = test_data['human_text'].tolist()

    print(f"G√©n√©ration de pr√©dictions pour {len(llm_texts)} exemples...")
    predictions = []

    for text in tqdm(llm_texts):
        try:
            pred = get_humanized_text(model, text)
            predictions.append(pred)
        except Exception as e:
            print(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
            predictions.append(text)  # Utiliser le texte original en cas d'erreur

    # Calculer les m√©triques
    metrics = calculate_simple_metrics(predictions, human_texts)

    # Afficher les m√©triques
    print("\nüìà M√âTRIQUES")
    print("-" * 50)
    print(f"Diff√©rence de longueur moyenne: {metrics['length_diff']:.4f}")
    print(f"Ratio moyen du nombre de mots: {metrics['word_count_ratio']:.4f}")
    print(f"Similarit√© lexicale moyenne: {metrics['word_overlap']:.4f}")

    # Calculer un score d'exactitude simplifi√© (bas√© sur la similarit√©)
    accuracy = metrics['word_overlap'] * 100
    print(f"Score d'exactitude approximatif: {accuracy:.2f}%")

    # Afficher quelques exemples
    samples = min(num_samples, len(llm_texts))
    print(f"\n‚ú® EXEMPLES ({samples})")
    print("-" * 50)

    for i in range(samples):
        print(f"\nExemple {i+1}:")
        print(f"Original:  {llm_texts[i]}")
        print(f"R√©f√©rence: {human_texts[i]}")
        print(f"Pr√©diction: {predictions[i]}")

    return {
        "metrics": metrics,
        "accuracy": accuracy,
        "predictions": predictions,
        "references": human_texts
    }
# === 4. Fonctions d'humanisation et d'√©valuation ===
def get_humanized_text(model, text: str, style_prefix: str = ""):
    """
    Humanise un texte donn√© en utilisant le mod√®le entra√Æn√©.

    Args:
        model: Instance de TextHumanizerModel
        text: Texte √† humaniser
        style_prefix: Pr√©fixe de style optionnel (ex: "[formel] ")

    Returns:
        str: Texte humanis√©
    """
    # Protection contre les entr√©es vides
    if not text or not text.strip():
        return text

    # Pr√©fixer avec le style si n√©cessaire
    prefixed_text = f"{style_prefix}{text}".strip()

    # Longueur adaptative pour √©viter les troncatures
    estimated_max_length = min(512, len(text.split()) * 2)

    try:
        # G√©n√©rer avec des param√®tres optimis√©s pour un texte naturel
        result = model.generate(
            prefixed_text,
            max_length=estimated_max_length,
            num_beams=4,
            temperature=0.8,  # L√©g√®rement al√©atoire pour plus de naturel
            top_k=50,
            top_p=0.95,  # √âchantillonnage nucleus pour plus de vari√©t√©
            no_repeat_ngram_size=3,  # √âviter les r√©p√©titions
            do_sample=True  # √âchantillonnage pour plus de diversit√©
        )

        return result if isinstance(result, str) else result[0]
    except Exception as e:
        print(f"‚ùå Erreur lors de l'humanisation: {str(e)}")
        return text  # Retour du texte original en cas d'√©chec

# Fonction de tokenisation simplifi√©e pour l'√©valuation
def simple_tokenize(text):
    """Tokenisation simple sans d√©pendances externes"""
    if not text:
        return []

    text = text.lower()
    # Remplacer la ponctuation par des espaces
    text = re.sub(r'[^\w\s]', ' ', text)
    # Diviser sur les espaces
    return [t for t in text.split() if t]

# Calcul de m√©triques d'√©valuation
def calculate_simple_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:
    """
    Calcule des m√©triques simples pour √©valuer la qualit√© de l'humanisation.
    """
    metrics = {}

    # Protection contre les listes vides
    if not predictions or not references:
        return {"error": "Donn√©es d'√©valuation vides"}

    try:
        # Diff√©rence de longueur (normalis√©e)
        len_diffs = [abs(len(p) - len(r)) / max(len(r), 1) for p, r in zip(predictions, references)]
        metrics['length_diff'] = np.mean(len_diffs)

        # Ratio du nombre de mots
        pred_word_counts = [len(simple_tokenize(p)) for p in predictions]
        ref_word_counts = [len(simple_tokenize(r)) for r in references]
        word_count_ratios = [p/max(r, 1) for p, r in zip(pred_word_counts, ref_word_counts)]
        metrics['word_count_ratio'] = np.mean(word_count_ratios)

        # Similarit√© lexicale simple (proportion de mots en commun)
        word_overlaps = []
        for p, r in zip(predictions, references):
            p_words = set(simple_tokenize(p))
            r_words = set(simple_tokenize(r))

            if not r_words:  # √âviter la division par z√©ro
                overlap = 0.0
            else:
                overlap = len(p_words.intersection(r_words)) / len(r_words)

            word_overlaps.append(overlap)

        metrics['word_overlap'] = np.mean(word_overlaps)

        return metrics
    except Exception as e:
        return {"error": f"Erreur lors du calcul des m√©triques: {str(e)}"}

# === 5. Gestion des donn√©es ===
# Cr√©ation d'un jeu de donn√©es pour l'entra√Ænement
def create_sample_dataset(output_path="data/humanized_pairs.csv", size=50):
    """Cr√©e un dataset d'exemple pour l'entra√Ænement avec plus d'exemples"""

    # Exemples de base
    data = [
        {"llm_text": "Les r√©sultats de l'analyse indiquent une corr√©lation positive entre les variables A et B avec un coefficient de 0.78.",
         "human_text": "Notre analyse montre que A et B sont fortement li√©s - on a trouv√© un coefficient de 0.78!"},
        {"llm_text": "La proc√©dure d'installation requiert les √©tapes suivantes: t√©l√©chargement du fichier, extraction des donn√©es et configuration des param√®tres initiaux.",
         "human_text": "Pour installer le programme, vous devez d'abord t√©l√©charger le fichier, puis extraire les donn√©es et enfin configurer vos param√®tres de d√©part."},
        {"llm_text": "L'√©tude d√©montre que la consommation r√©guli√®re de fruits et l√©gumes est associ√©e √† une r√©duction de 23% du risque de maladies cardiovasculaires.",
         "human_text": "D'apr√®s l'√©tude, manger des fruits et l√©gumes r√©guli√®rement pourrait r√©duire votre risque de probl√®mes cardiaques d'environ 23%."},
        {"llm_text": "Il est recommand√© d'effectuer une sauvegarde des donn√©es avant de proc√©der √† la mise √† jour du syst√®me d'exploitation.",
         "human_text": "Je vous conseille vraiment de faire une sauvegarde de vos donn√©es avant de mettre √† jour votre syst√®me."},
        {"llm_text": "En conclusion, les donn√©es ne permettent pas de rejeter l'hypoth√®se nulle avec un niveau de signification de 0.05.",
         "human_text": "En fin de compte, nos r√©sultats ne sont pas assez concluants pour rejeter l'hypoth√®se de d√©part (avec un seuil de 0.05)."},
        {"llm_text": "La temp√©rature optimale de fonctionnement se situe entre 20 et 25 degr√©s Celsius.",
         "human_text": "L'appareil fonctionne mieux quand il fait entre 20 et 25 degr√©s dans la pi√®ce."},
        {"llm_text": "L'analyse des commentaires r√©v√®le un sentiment g√©n√©ralement positif avec 68% d'opinions favorables.",
         "human_text": "En lisant les commentaires, on voit que la plupart des gens sont contents - environ deux tiers ont laiss√© des avis positifs!"},
        {"llm_text": "La m√©thode propos√©e pr√©sente une am√©lioration de l'efficacit√© de 15% par rapport aux approches conventionnelles.",
         "human_text": "Notre nouvelle m√©thode est 15% plus efficace que les techniques habituelles."},
        {"llm_text": "Les utilisateurs doivent s'authentifier via le portail s√©curis√© avant d'acc√©der aux ressources prot√©g√©es.",
         "human_text": "Vous devez vous connecter sur le portail s√©curis√© avant de pouvoir acc√©der √† ces ressources."},
        {"llm_text": "La dur√©e estim√©e pour compl√©ter cette t√¢che est approximativement de 3 heures et 45 minutes.",
         "human_text": "Ce travail devrait vous prendre environ 3h45 √† terminer."},
        # Nouveaux exemples pour enrichir le dataset
        {"llm_text": "Veuillez noter que l'√©v√©nement a √©t√© reprogramm√© √† une date ult√©rieure en raison de circonstances impr√©vues.",
         "human_text": "On a d√ª d√©caler l'√©v√©nement √† plus tard √† cause d'un impr√©vu, d√©sol√©!"},
        {"llm_text": "Il est imp√©ratif de proc√©der √† une v√©rification minutieuse des documents avant leur soumission d√©finitive.",
         "human_text": "Il faut bien v√©rifier vos documents avant de les soumettre d√©finitivement."},
        {"llm_text": "Les participants sont pri√©s de confirmer leur pr√©sence au minimum 48 heures avant le d√©but de la conf√©rence.",
         "human_text": "Merci de nous dire si vous venez au moins 2 jours avant la conf√©rence!"},
        {"llm_text": "Le rapport financier trimestriel indique une augmentation des revenus de 7.2% par rapport √† la p√©riode pr√©c√©dente.",
         "human_text": "On a vu nos revenus grimper de 7.2% ce trimestre par rapport au pr√©c√©dent!"},
        {"llm_text": "Suite √† l'analyse des donn√©es recueillies, il appara√Æt que la strat√©gie A est significativement plus performante que la strat√©gie B.",
         "human_text": "Apr√®s avoir analys√© toutes les donn√©es, on voit clairement que la strat√©gie A marche bien mieux que la B."}
    ]

    # Nombre de r√©p√©titions n√©cessaires pour atteindre la taille demand√©e
    repetitions = max(1, size // len(data) + 1)

    # Augmenter les donn√©es pour avoir un dataset plus grand
    extended_data = []
    for _ in range(repetitions):
        extended_data.extend(data)

    # Limiter √† la taille demand√©e
    extended_data = extended_data[:size]

    # Sauvegarder avec gestion des erreurs
    try:
        df = pd.DataFrame(extended_data)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        df.to_csv(output_path, index=False)
        print(f"‚úÖ Dataset d'exemple cr√©√©: {output_path} ({len(df)} exemples)")
        return df
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation du dataset: {str(e)}")
        return pd.DataFrame(data)

# === 6. Fonction d'entra√Ænement optimis√©e ===
def train_model(
    model_name: str = "t5-small",
    data_path: str = "data/humanized_pairs.csv",
    output_dir: str = "model/saved_model",
    num_epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 5e-5,
    device: Optional[str] = None,
    use_styles: bool = False,
    checkpoint_dir: str = "model/checkpoints",
    max_length: int = 256,
    auto_save: bool = True,
    save_interval: int = 1,
    show_metrics: bool = True,
):
    start_time = time.time()

    if not device:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    print(f"üöÄ Entra√Ænement sur {device}")
    print(f"   Mod√®le: {model_name}")
    print(f"   Donn√©es: {data_path}")
    print(f"   √âpoque: {num_epochs}")
    print(f"   Batch size: {batch_size}")

    try:
        # Charger mod√®le et tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        if device == "cuda":
            model = model.half()
        model.to(device)

        # Charger donn√©es
        if data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        elif data_path.endswith('.jsonl') or data_path.endswith('.json'):
            df = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))
        else:
            raise ValueError("Format de fichier non pris en charge. Utilisez CSV ou JSONL.")

        required_columns = ['llm_text', 'human_text']
        if not all(col in df.columns for col in required_columns):
            alternatives = {
                'llm_text': ['machine_text', 'bot_text', 'ai_text', 'generated_text'],
                'human_text': ['humanized_text', 'natural_text', 'target_text']
            }
            for req_col, alt_cols in alternatives.items():
                if req_col not in df.columns:
                    for alt_col in alt_cols:
                        if alt_col in df.columns:
                            df[req_col] = df[alt_col]
                            break

        if not all(col in df.columns for col in required_columns):
            raise ValueError(f"Le dataset doit contenir les colonnes {required_columns}")

        llm_texts = df['llm_text'].tolist()
        human_texts = df['human_text'].tolist()

        print(f"‚úÖ Donn√©es charg√©es: {len(llm_texts)} paires")

        if use_styles:
            styles = ["formel", "amical", "enthousiaste"]
            styled_llm_texts = []
            styled_human_texts = []

            print("G√©n√©ration de variations de style...")
            for i, (llm_text, human_text) in enumerate(zip(llm_texts, human_texts)):
                styled_llm_texts.append(llm_text)
                styled_human_texts.append(human_text)
                if i % 2 == 0:
                    style = styles[i % len(styles)]
                    styled_llm_texts.append(f"[{style}] {llm_text}")
                    styled_human_texts.append(human_text)

            llm_texts, human_texts = styled_llm_texts, styled_human_texts
            print(f"‚úÖ Dataset enrichi avec styles: {len(llm_texts)} exemples")

        train_texts, val_texts, train_human, val_human = train_test_split(
            llm_texts, human_texts, test_size=0.2, random_state=42
        )

        train_dataset = HumanizerDataset(train_texts, train_human, tokenizer, max_length=max_length)
        val_dataset = HumanizerDataset(val_texts, val_human, tokenizer, max_length=max_length)

        training_args = Seq2SeqTrainingArguments(
            output_dir=checkpoint_dir,
            eval_steps=100,
            save_steps=100,
            learning_rate=learning_rate,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=2,
            num_train_epochs=num_epochs,
            predict_with_generate=True,
            fp16=device == "cuda",
            logging_dir=os.path.join(output_dir, "logs"),
            logging_steps=10,
            report_to="none",

        )

        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding="max_length",
            max_length=max_length
        )

        def compute_metrics(eval_preds):
            preds, labels = eval_preds
            predictions = tokenizer.batch_decode(preds, skip_special_tokens=True)
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            references = tokenizer.batch_decode(labels, skip_special_tokens=True)
            metrics = calculate_simple_metrics(predictions, references)
            return metrics

        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics
        )

        # --- AJOUTER METRICS CALLBACK ---
        class MetricsCallback(transformers.TrainerCallback):
            def __init__(self, trainer, eval_dataset, tokenizer, output_dir, save_interval):
                self.trainer = trainer
                self.eval_dataset = eval_dataset
                self.tokenizer = tokenizer
                self.output_dir = output_dir
                self.save_interval = save_interval
                self.best_metric = float('inf')
                self.step_metrics = {}

            def on_evaluate(self, args, state, control, metrics, **kwargs):
                self.step_metrics[state.global_step] = metrics

                if show_metrics:
                    print(f"\nüìä M√âTRIQUES √Ä L'√âTAPE {state.global_step}")
                    print("-" * 50)
                    for key, value in metrics.items():
                        if isinstance(value, float):
                            print(f"{key}: {value:.4f}")
                        else:
                            print(f"{key}: {value}")

                if auto_save and (state.global_step % (100 * save_interval) == 0):
                    test_text = "L'analyse des donn√©es indique une corr√©lation significative entre les variables √©tudi√©es."
                    temp_dir = os.path.join(output_dir, "temp_checkpoint")
                    os.makedirs(temp_dir, exist_ok=True)
                    model.save_pretrained(temp_dir)
                    tokenizer.save_pretrained(temp_dir)
                    try:
                        test_model = TextHumanizerModel.load_model(temp_dir, device)
                        humanized = get_humanized_text(test_model, test_text)
                        checkpoint_dir = os.path.join(output_dir, f"checkpoint-{state.global_step}")
                        os.makedirs(checkpoint_dir, exist_ok=True)
                        model.save_pretrained(checkpoint_dir)
                        tokenizer.save_pretrained(checkpoint_dir)
                        with open(os.path.join(checkpoint_dir, "model_info.json"), "w") as f:
                            json.dump({
                                "step": state.global_step,
                                "metrics": metrics,
                                "test_output": humanized,
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            }, f, indent=2)
                        print(f"‚úÖ Mod√®le test√© et sauvegard√© √† l'√©tape {state.global_step}")
                        print(f"   Exemple de sortie: \"{humanized}\"")
                    except Exception as e:
                        print(f"‚ùå Erreur lors du test du mod√®le: {str(e)}")

        trainer.add_callback(MetricsCallback(trainer, val_dataset, tokenizer, output_dir, save_interval))

        # --- D√©marrer l'entra√Ænement ---
        try:
            trainer.train()

            # --- √âvaluation finale ---
            print("\nüîç √âVALUATION FINALE DU MOD√àLE")
            test_df = pd.DataFrame({
                'llm_text': val_texts[:10],
                'human_text': val_human[:10]
            })
            humanizer = TextHumanizerModel.load_model(output_dir, device)
            eval_results = evaluate_model(humanizer, test_df)

            with open(os.path.join(output_dir, "evaluation_results.json"), "w") as f:
                serializable_results = {
                    "metrics": eval_results["metrics"],
                    "accuracy": eval_results["accuracy"],
                    "examples": [
                        {
                            "original": val_texts[i],
                            "reference": val_human[i],
                            "prediction": eval_results["predictions"][i]
                        }
                        for i in range(min(5, len(val_texts)))
                    ]
                }
                json.dump(serializable_results, f, indent=2)
            print(f"‚úÖ R√©sultats d'√©valuation sauvegard√©s dans {output_dir}/evaluation_results.json")

        except KeyboardInterrupt:
            print("Entra√Ænement interrompu manuellement.")

        # Sauvegarde finale
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)

        metadata = {
            "model_name": model_name,
            "base_model": model_name,
            "dataset_size": len(llm_texts),
            "epochs": num_epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "max_length": max_length,
            "training_time": f"{(time.time() - start_time) / 60:.2f} minutes",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(os.path.join(output_dir, "model_info.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        print(f"‚úÖ Entra√Ænement termin√© en {(time.time() - start_time) / 60:.2f} minutes")
        print(f"‚úÖ Mod√®le sauvegard√© dans {output_dir}")

        return {"status": "success", "output_dir": output_dir, "training_time": time.time() - start_time}

    except Exception as e:
        print(f"‚ùå Erreur lors de l'entra√Ænement: {str(e)}")
        return {"status": "error", "error": str(e)}
# Ajouter cette fonction pour tester facilement le mod√®le
def test_model_accuracy(model_path="model/saved_model", custom_test=None):
    """
    Teste un mod√®le entra√Æn√© et affiche son exactitude et autres m√©triques

    Args:
        model_path: Chemin vers le mod√®le entra√Æn√©
        custom_test: Texte personnalis√© √† tester
    """
    try:
        # Charger le mod√®le
        print(f"Chargement du mod√®le depuis {model_path}...")
        model = TextHumanizerModel.load_model(model_path)

        # √âvaluer sur des exemples standard
        eval_results = evaluate_model(model)

        if custom_test:
            print("\n‚úèÔ∏è TEST PERSONNALIS√â")
            print("-" * 50)
            print(f"Original: {custom_test}")
            humanized = get_humanized_text(model, custom_test)
            print(f"Humanis√©: {humanized}")

        # Interface interactive simple
        print("\nüîÑ TESTER D'AUTRES EXEMPLES")
        print("-" * 50)
        print("Entrez du texte √† humaniser (ou 'q' pour quitter):")

        while True:
            user_input = input("\nTexte: ")
            if user_input.lower() == 'q':
                break

            if user_input:
                try:
                    humanized = get_humanized_text(model, user_input)
                    print(f"\nR√©sultat: {humanized}")
                except Exception as e:
                    print(f"‚ùå Erreur: {str(e)}")
            else:
                print("Texte vide. Veuillez entrer un texte.")

        return eval_results

    except Exception as e:
        print(f"‚ùå Erreur lors du test du mod√®le: {str(e)}")
        return None

# === 7. API REST ===
app = FastAPI(title="API d'Humanisation de Texte LLM")

class TextRequest(BaseModel):
    text: str
    style: Optional[str] = ""

class TextResponse(BaseModel):
    original: str
    humanized: str
    processing_time: float

# Variable globale pour stocker le mod√®le
global_model = None
model_loading = False
model_lock = threading.Lock()

def load_model_bg(model_path: str, device: Optional[str] = None):
    """Charge le mod√®le en arri√®re-plan"""
    global global_model, model_loading

    try:
        print(f"Chargement du mod√®le depuis {model_path}...")
        model = TextHumanizerModel.load_model(model_path, device)

        with model_lock:
            global_model = model
            model_loading = False

        print("‚úÖ Mod√®le charg√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
        model_loading = False

@app.post("/humanize", response_model=TextResponse)
async def humanize_text(request: TextRequest, background_tasks: BackgroundTasks):
    """Endpoint pour humaniser du texte"""
    global global_model, model_loading

    # V√©rifier si le mod√®le est charg√©
    if global_model is None:
        if not model_loading:
            model_loading = True
            # Charger le mod√®le en arri√®re-plan
            background_tasks.add_task(load_model_bg, "model/saved_model")

        raise HTTPException(
            status_code=503,
            detail="Le mod√®le est en cours de chargement. Veuillez r√©essayer dans quelques instants."
        )

    # Humaniser le texte
    start_time = time.time()
    try:
        humanized = get_humanized_text(global_model, request.text, request.style)
        processing_time = time.time() - start_time

        return {
            "original": request.text,
            "humanized": humanized,
            "processing_time": processing_time
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de l'humanisation: {str(e)}")

# === 8. Interface utilisateur dans Colab ===
def create_ui(model_path="model/saved_model"):
    """Cr√©e une interface utilisateur simple dans Colab"""
    global global_model

    # Charger le mod√®le s'il n'est pas d√©j√† charg√©
    if global_model is None:
        try:
            global_model = TextHumanizerModel.load_model(model_path)
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
            print("Continuer avec un mod√®le par d√©faut...")
            global_model = TextHumanizerModel()

    # Cr√©er les widgets
    style_dropdown = widgets.Dropdown(
        options=[
            ('Standard', ''),
            ('Formel', '[formel]'),
            ('Amical', '[amical]'),
            ('Enthousiaste', '[enthousiaste]')
        ],
        description='Style:',
        disabled=False,
    )

    text_area = widgets.Textarea(
        value='Exemple: La proc√©dure d\'installation requiert les √©tapes suivantes: t√©l√©chargement du fichier, extraction des donn√©es et configuration des param√®tres initiaux.',
        placeholder='Entrez le texte √† humaniser',
        description='Texte:',
        disabled=False,
        layout=widgets.Layout(width='100%', height='100px')
    )

    output_area = widgets.Output(layout=widgets.Layout(border='1px solid black', padding='10px', width='100%'))

    humanize_button = widgets.Button(
        description='Humaniser',
        disabled=False,
        button_style='primary',
        tooltip='Cliquez pour humaniser le texte',
        icon='magic'
    )

    # Fonction pour humaniser le texte
    def on_button_click(b):
        with output_area:
            clear_output()
            print("‚è≥ Humanisation en cours...")

            try:
                start_time = time.time()
                humanized = get_humanized_text(global_model, text_area.value, style_dropdown.value)
                elapsed = time.time() - start_time

                clear_output()
                print("‚úÖ Texte humanis√©:")
                print(f"\nüìù Original: \"{text_area.value}\"")
                print(f"\n‚ú® Humanis√©: \"{humanized}\"")
                print(f"\n‚è±Ô∏è Temps de traitement: {elapsed:.2f} secondes")
            except Exception as e:
                clear_output()
                print(f"‚ùå Erreur: {str(e)}")

    # Associer la fonction au bouton
    humanize_button.on_click(on_button_click)

    # Afficher l'interface
    display(HTML("<h2>Humaniseur de Texte LLM</h2>"))
    display(text_area)
    display(style_dropdown)
    display(humanize_button)
    display(output_area)

# === 9. Fonction principale pour lancer le service ===
def start_api_server():
    """D√©marre le serveur API"""
    try:
        # Configurer le tunnel ngrok
        from pyngrok import ngrok

        # D√©marrer le serveur uvicorn dans un thread s√©par√©
        port = 8000
        thread = threading.Thread(target=lambda: uvicorn.run(app, host="127.0.0.1", port=port))
        thread.daemon = True
        thread.start()

        # Cr√©er un tunnel ngrok
        public_url = ngrok.connect(port).public_url
        print(f"‚úÖ API accessible √†: {public_url}")
        print("Documentation API: {}/docs".format(public_url))

        # Charger le mod√®le en arri√®re-plan
        global model_loading
        model_loading = True
        threading.Thread(target=lambda: load_model_bg("model/saved_model")).start()

    except Exception as e:
        print(f"‚ùå Erreur lors du d√©marrage du serveur: {str(e)}")

# === 10. Fonction principale ===
def run_demo():
    """
    Ex√©cute la d√©monstration compl√®te:
    1. V√©rifie le GPU
    2. Cr√©e la structure de r√©pertoires
    3. Cr√©e un dataset d'exemple
    4. Entra√Æne le mod√®le
    5. Lance l'interface utilisateur
    """
    print("üöÄ D√©marrage de la d√©monstration d'humanisation de texte LLM")

    # V√©rifier le GPU
    has_gpu = check_gpu()

    # Cr√©er la structure de r√©pertoires
    setup_directories()

    # Cr√©er un dataset d'exemple
    df = create_sample_dataset(size=100)

    # Demander √† l'utilisateur s'il souhaite entra√Æner le mod√®le
    train_widget = widgets.Button(
        description='Entra√Æner le mod√®le',
        button_style='primary',
        tooltip='Cliquez pour entra√Æner le mod√®le'
    )

    demo_widget = widgets.Button(
        description='Utiliser le mod√®le pr√©-entra√Æn√©',
        button_style='info',
        tooltip='Cliquez pour utiliser le mod√®le pr√©-entra√Æn√©'
    )

    output_widget = widgets.Output()

    def on_train_click(b):
        with output_widget:
            clear_output()
            print("‚è≥ Entra√Ænement du mod√®le...")

            # Adapter les param√®tres selon la disponibilit√© du GPU
            batch_size = 8 if has_gpu else 4
            model_name = "t5-small"  # Mod√®le plus petit pour Colab

            result = train_model(
                model_name=model_name,
                data_path="data/humanized_pairs.csv",
                num_epochs=3,
                batch_size=batch_size,
                learning_rate=5e-5,
                use_styles=True,
                max_length=256  # R√©duire pour √©conomiser la m√©moire
            )

            if result["status"] == "success":
                print("\n‚úÖ Entra√Ænement termin√©")
                print("Lancement de l'interface utilisateur...")
                create_ui("model/saved_model")
            else:
                print(f"\n‚ùå Erreur lors de l'entra√Ænement: {result.get('error', 'Erreur inconnue')}")

    def on_demo_click(b):
        with output_widget:
            clear_output()
            print("‚è≥ Chargement du mod√®le pr√©-entra√Æn√©...")

            # Utiliser le mod√®le de base pour la d√©monstration
            global global_model
            global_model = TextHumanizerModel()

            print("\n‚úÖ Mod√®le pr√™t")
            print("Lancement de l'interface utilisateur...")
            create_ui()

    # Associer les fonctions aux boutons
    train_widget.on_click(on_train_click)
    demo_widget.on_click(on_demo_click)

    # Afficher les boutons
    display(HTML("<h2>Humaniseur de Texte LLM - D√©marrage</h2>"))
    display(HTML("<p>Choisissez une option pour continuer:</p>"))
    display(widgets.HBox([train_widget, demo_widget]))
    display(output_widget)

# === 11. Point d'entr√©e principal ===
if __name__ == "__main__":
    # Montage de Google Drive (optionnel)
    mount_drive = False
    if mount_drive:
        try:
            drive.mount('/content/drive')
            print("‚úÖ Google Drive mont√© avec succ√®s")
        except Exception as e:
            print(f"‚ùå Erreur lors du montage de Google Drive: {str(e)}")

    # Ex√©cuter la d√©monstration
    run_demo()

create_ui("model/saved_model")