# -*- coding: utf-8 -*-
"""LLM_Human.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-FbYCTUqHJMrPzx1gQySSbOgqhFqnqk
"""

!pip show transformers

!pip install --upgrade transformers

import transformers
print(transformers.__version__)

!pip install transformers datasets

# Text Humanizer - Transformer du texte LLM en texte plus humain
# Notebook Google Colab optimisé

# === 1. Installation et configuration ===
# Installation silencieuse des dépendances nécessaires
!pip install -q fastapi uvicorn transformers datasets tqdm ipywidgets pyngrok

# Import des bibliothèques nécessaires
import os
import torch
import numpy as np
import pandas as pd
import re
import json
import time
import threading
from typing import List, Dict, Optional, Union
from datetime import datetime
from tqdm.notebook import tqdm
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from google.colab import output, drive
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from functools import lru_cache
import transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

# Vérification de la disponibilité du GPU et affichage des informations
def check_gpu():
    """Vérifie la disponibilité du GPU et affiche les informations"""
    if torch.cuda.is_available():
        print(f" GPU disponible: {torch.cuda.get_device_name(0)}")
        # Afficher la mémoire GPU disponible
        print(f"Mémoire GPU totale: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        !nvidia-smi
        return True
    else:
        print(" Pas de GPU détecté. L'entraînement sera plus lent sur CPU.")
        return False

# Création de la structure de répertoires avec gestion des erreurs
def setup_directories():
    """Crée la structure de répertoires nécessaire"""
    directories = ["model/saved_model", "data", "exports"]
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception as e:
            print(f"Erreur lors de la création du répertoire {directory}: {e}")
    print(" Structure de répertoires créée")

# === 2. Définition du modèle d'humanisation ===
class TextHumanizerModel:
    """Modèle pour transformer du texte LLM en texte plus humain"""

    def __init__(self, model_name: str = "t5-small", device: Optional[str] = None, max_length: int = 512):
        """
        Initialise le modèle de ré-humanisation.

        Args:
            model_name: Nom ou chemin du modèle Hugging Face
            device: Appareil sur lequel exécuter le modèle ('cpu' ou 'cuda')
            max_length: Longueur maximale des entrées/sorties
        """
        self.model_name = model_name
        self.max_length = max_length
        self.cache = {}  # Cache simple pour les résultats fréquents

        # Déterminer le dispositif (GPU par défaut dans Colab si disponible)
        if device:
            self.device = device
        else:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Chargement du modèle avec gestion des erreurs
        try:
            print(f"Chargement du modèle {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

            # Optimisations pour Google Colab
            if self.device == "cuda":
                # Utiliser la précision mixte pour économiser de la mémoire
                self.model = self.model.half()  # Conversion en FP16

            self.model.to(self.device)
            print(f"Modèle chargé sur {self.device}")
        except Exception as e:
            print(f" Erreur lors du chargement du modèle {model_name}: {str(e)}")
            raise

    def save_model(self, output_dir: str):
        """
        Sauvegarde le modèle et le tokenizer avec métadonnées.

        Args:
            output_dir: Chemin du répertoire où sauvegarder le modèle
        """
        try:
            os.makedirs(output_dir, exist_ok=True)

            # Sauvegarde avec barre de progression
            print("Sauvegarde du modèle...")
            self.model.save_pretrained(output_dir)
            self.tokenizer.save_pretrained(output_dir)

            # Sauvegarde des métadonnées avec timestamp
            metadata = {
                "model_name": self.model_name,
                "device": self.device,
                "max_length": self.max_length,
                "saved_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            with open(os.path.join(output_dir, "model_info.json"), "w") as f:
                json.dump(metadata, f, indent=2)

            print(f"Modèle sauvegardé dans {output_dir}")
            return True
        except Exception as e:
            print(f" Erreur lors de la sauvegarde du modèle: {str(e)}")
            return False

    @classmethod
    def load_model(cls, model_path: str, device: Optional[str] = None):
        """
        Charge un modèle sauvegardé avec gestion des erreurs.

        Args:
            model_path: Chemin vers le modèle sauvegardé
            device: Appareil sur lequel charger le modèle

        Returns:
            TextHumanizerModel: Instance du modèle chargé
        """
        try:
            # Créer une instance vide
            instance = cls.__new__(cls)

            # Initialiser manuellement les attributs nécessaires
            instance.model_path = model_path
            instance.cache = {}

            # Configurer le dispositif
            if device:
                instance.device = device
            else:
                instance.device = "cuda" if torch.cuda.is_available() else "cpu"

            # Charger les métadonnées
            metadata = {}
            info_path_json = os.path.join(model_path, "model_info.json")
            info_path_txt = os.path.join(model_path, "model_info.txt")

            if os.path.exists(info_path_json):
                with open(info_path_json, "r") as f:
                    metadata = json.load(f)
                instance.model_name = metadata.get("model_name", "custom_model")
                instance.max_length = metadata.get("max_length", 512)
            elif os.path.exists(info_path_txt):
                with open(info_path_txt, "r") as f:
                    for line in f:
                        key, value = line.strip().split(": ", 1)
                        metadata[key] = value
                instance.model_name = metadata.get("model_name", "custom_model")
                instance.max_length = 512
            else:
                instance.model_name = "custom_model"
                instance.max_length = 512

            # Charger le tokenizer et le modèle avec barre de progression
            print(f"Chargement du modèle depuis {model_path}...")
            instance.tokenizer = AutoTokenizer.from_pretrained(model_path)
            instance.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

            # Optimisations pour Google Colab
            if instance.device == "cuda":
                # Utiliser la précision mixte pour économiser de la mémoire
                instance.model = instance.model.half()

            instance.model.to(instance.device)
            print(f"Modèle chargé sur {instance.device}")

            return instance
        except Exception as e:
            print(f" Erreur lors du chargement du modèle: {str(e)}")
            raise

    def tokenize(self, text: Union[str, List[str]]):
        """
        Tokenise le texte d'entrée avec gestion de la longueur maximale.

        Args:
            text: Texte ou liste de textes à tokeniser

        Returns:
            Tokens encodés
        """
        return self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

    @lru_cache(maxsize=128)  # Cache pour les requêtes fréquentes
    def _generate_cached(self, input_text: str, **kwargs):
        """Version mise en cache de la génération pour les textes fréquents"""
        # Tokeniser
        inputs = self.tokenize([input_text])

        # Générer avec contrôle d'erreurs
        try:
            with torch.no_grad():
                output_ids = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **kwargs
                )

            # Décoder
            return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        except Exception as e:
            print(f" Erreur lors de la génération pour '{input_text[:30]}...': {str(e)}")
            return input_text  # Retourner le texte original en cas d'erreur

    def generate(self, input_text: Union[str, List[str]], **kwargs):
        """
        Génère du texte humanisé à partir du texte d'entrée avec optimisations.

        Args:
            input_text: Texte ou liste de textes à humaniser
            **kwargs: Arguments supplémentaires pour la génération

        Returns:
            List[str]: Liste des textes humanisés
        """
        # Préparer les entrées
        single_input = isinstance(input_text, str)
        if single_input:
            input_text = [input_text]

        # Préfixer les entrées pour indiquer la tâche
        prefixed_inputs = [f"humanize: {text}" for text in input_text]

        # Paramètres par défaut pour la génération
        generation_kwargs = {
            "max_length": min(self.max_length, 512),
            "num_beams": 4,
            "length_penalty": 1.0,
            "early_stopping": True,
            "no_repeat_ngram_size": 3,
            "temperature": 0.8,
            "top_k": 50,
            "top_p": 0.95,
            "do_sample": True
        }

        # Mettre à jour avec les kwargs fournis
        generation_kwargs.update(kwargs)

        # Générer les réponses
        results = []

        for text in prefixed_inputs:
            # Utiliser la version mise en cache pour les textes individuels
            result = self._generate_cached(text, **generation_kwargs)
            results.append(result)

        # Retourner un seul résultat si l'entrée était une chaîne
        if single_input:
            return results[0]
        return results

# === 3. Définition du dataset pour l'entraînement ===
class HumanizerDataset(Dataset):
    """Dataset pour l'entraînement du modèle de ré-humanisation."""

    def __init__(self, texts, human_texts, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.texts = texts
        self.human_texts = human_texts
        self.max_length = max_length

        # Préfixer les entrées pour indiquer la tâche
        self.input_texts = [f"humanize: {text}" for text in texts]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Tokeniser l'entrée avec gestion d'erreurs
        try:
            # Tokeniser l'entrée
            input_encoding = self.tokenizer(
                self.input_texts[idx],
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )

            # Tokeniser la sortie
            with self.tokenizer.as_target_tokenizer():
                target_encoding = self.tokenizer(
                    self.human_texts[idx],
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt"
                )

            # Préparer les entrées pour l'entraînement
            input_ids = input_encoding["input_ids"].squeeze()
            attention_mask = input_encoding["attention_mask"].squeeze()
            labels = target_encoding["input_ids"].squeeze()

            # Remplacer les tokens de padding par -100 pour les ignorer
            labels[labels == self.tokenizer.pad_token_id] = -100

            return {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": labels
            }
        except Exception as e:
            # En cas d'erreur, retourner un exemple vide (sera ignoré)
            print(f" Erreur lors de la tokenisation de l'exemple {idx}: {str(e)}")
            dummy_tensor = torch.zeros(self.max_length, dtype=torch.long)
            return {
                "input_ids": dummy_tensor,
                "attention_mask": dummy_tensor,
                "labels": dummy_tensor
            }
def evaluate_model(model, test_data=None, num_samples=10):
    """
    Évalue le modèle et affiche les métriques pertinentes

    Args:
        model: Le modèle TextHumanizerModel à évaluer
        test_data: DataFrame contenant des paires de textes pour l'évaluation
        num_samples: Nombre d'exemples à afficher
    """
    print(" ÉVALUATION DU MODÈLE")
    print("=" * 50)

    # Si aucune donnée de test n'est fournie, utiliser quelques exemples standards
    if test_data is None:
        test_data = pd.DataFrame({
            'llm_text': [
                "Les résultats de l'analyse indiquent une corrélation positive entre les variables A et B avec un coefficient de 0.78.",
                "La procédure d'installation requiert les étapes suivantes: téléchargement du fichier, extraction des données et configuration des paramètres initiaux.",
                "L'étude démontre que la consommation régulière de fruits et légumes est associée à une réduction de 23% du risque de maladies cardiovasculaires."
            ],
            'human_text': [
                "Notre analyse montre que A et B sont fortement liés - on a trouvé un coefficient de 0.78!",
                "Pour installer le programme, vous devez d'abord télécharger le fichier, puis extraire les données et enfin configurer vos paramètres de départ.",
                "D'après l'étude, manger des fruits et légumes régulièrement pourrait réduire votre risque de problèmes cardiaques d'environ 23%."
            ]
        })

    # Générer des prédictions
    llm_texts = test_data['llm_text'].tolist()
    human_texts = test_data['human_text'].tolist()

    print(f"Génération de prédictions pour {len(llm_texts)} exemples...")
    predictions = []

    for text in tqdm(llm_texts):
        try:
            pred = get_humanized_text(model, text)
            predictions.append(pred)
        except Exception as e:
            print(f" Erreur lors de la prédiction: {str(e)}")
            predictions.append(text)  # Utiliser le texte original en cas d'erreur

    # Calculer les métriques
    metrics = calculate_simple_metrics(predictions, human_texts)

    # Afficher les métriques
    print("\n MÉTRIQUES")
    print("-" * 50)
    print(f"Différence de longueur moyenne: {metrics['length_diff']:.4f}")
    print(f"Ratio moyen du nombre de mots: {metrics['word_count_ratio']:.4f}")
    print(f"Similarité lexicale moyenne: {metrics['word_overlap']:.4f}")

    # Calculer un score d'exactitude simplifié (basé sur la similarité)
    accuracy = metrics['word_overlap'] * 100
    print(f"Score d'exactitude approximatif: {accuracy:.2f}%")

    # Afficher quelques exemples
    samples = min(num_samples, len(llm_texts))
    print(f"\n EXEMPLES ({samples})")
    print("-" * 50)

    for i in range(samples):
        print(f"\nExemple {i+1}:")
        print(f"Original:  {llm_texts[i]}")
        print(f"Référence: {human_texts[i]}")
        print(f"Prédiction: {predictions[i]}")

    return {
        "metrics": metrics,
        "accuracy": accuracy,
        "predictions": predictions,
        "references": human_texts
    }
# === 4. Fonctions d'humanisation et d'évaluation ===
def get_humanized_text(model, text: str, style_prefix: str = ""):
    """
    Humanise un texte donné en utilisant le modèle entraîné.

    Args:
        model: Instance de TextHumanizerModel
        text: Texte à humaniser
        style_prefix: Préfixe de style optionnel (ex: "[formel] ")

    Returns:
        str: Texte humanisé
    """
    # Protection contre les entrées vides
    if not text or not text.strip():
        return text

    # Préfixer avec le style si nécessaire
    prefixed_text = f"{style_prefix}{text}".strip()

    # Longueur adaptative pour éviter les troncatures
    estimated_max_length = min(512, len(text.split()) * 2)

    try:
        # Générer avec des paramètres optimisés pour un texte naturel
        result = model.generate(
            prefixed_text,
            max_length=estimated_max_length,
            num_beams=4,
            temperature=0.8,  # Légèrement aléatoire pour plus de naturel
            top_k=50,
            top_p=0.95,  # Échantillonnage nucleus pour plus de variété
            no_repeat_ngram_size=3,  # Éviter les répétitions
            do_sample=True  # Échantillonnage pour plus de diversité
        )

        return result if isinstance(result, str) else result[0]
    except Exception as e:
        print(f" Erreur lors de l'humanisation: {str(e)}")
        return text  # Retour du texte original en cas d'échec

# Fonction de tokenisation simplifiée pour l'évaluation
def simple_tokenize(text):
    """Tokenisation simple sans dépendances externes"""
    if not text:
        return []

    text = text.lower()
    # Remplacer la ponctuation par des espaces
    text = re.sub(r'[^\w\s]', ' ', text)
    # Diviser sur les espaces
    return [t for t in text.split() if t]

# Calcul de métriques d'évaluation
def calculate_simple_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:
    """
    Calcule des métriques simples pour évaluer la qualité de l'humanisation.
    """
    metrics = {}

    # Protection contre les listes vides
    if not predictions or not references:
        return {"error": "Données d'évaluation vides"}

    try:
        # Différence de longueur (normalisée)
        len_diffs = [abs(len(p) - len(r)) / max(len(r), 1) for p, r in zip(predictions, references)]
        metrics['length_diff'] = np.mean(len_diffs)

        # Ratio du nombre de mots
        pred_word_counts = [len(simple_tokenize(p)) for p in predictions]
        ref_word_counts = [len(simple_tokenize(r)) for r in references]
        word_count_ratios = [p/max(r, 1) for p, r in zip(pred_word_counts, ref_word_counts)]
        metrics['word_count_ratio'] = np.mean(word_count_ratios)

        # Similarité lexicale simple (proportion de mots en commun)
        word_overlaps = []
        for p, r in zip(predictions, references):
            p_words = set(simple_tokenize(p))
            r_words = set(simple_tokenize(r))

            if not r_words:  # Éviter la division par zéro
                overlap = 0.0
            else:
                overlap = len(p_words.intersection(r_words)) / len(r_words)

            word_overlaps.append(overlap)

        metrics['word_overlap'] = np.mean(word_overlaps)

        return metrics
    except Exception as e:
        return {"error": f"Erreur lors du calcul des métriques: {str(e)}"}

# === 5. Gestion des données ===
# Création d'un jeu de données pour l'entraînement
def create_sample_dataset(output_path="data/humanized_pairs.csv", size=50):
    """Crée un dataset d'exemple pour l'entraînement avec plus d'exemples"""

    # Exemples de base
    data = [
        {"llm_text": "Les résultats de l'analyse indiquent une corrélation positive entre les variables A et B avec un coefficient de 0.78.",
         "human_text": "Notre analyse montre que A et B sont fortement liés - on a trouvé un coefficient de 0.78!"},
        {"llm_text": "La procédure d'installation requiert les étapes suivantes: téléchargement du fichier, extraction des données et configuration des paramètres initiaux.",
         "human_text": "Pour installer le programme, vous devez d'abord télécharger le fichier, puis extraire les données et enfin configurer vos paramètres de départ."},
        {"llm_text": "L'étude démontre que la consommation régulière de fruits et légumes est associée à une réduction de 23% du risque de maladies cardiovasculaires.",
         "human_text": "D'après l'étude, manger des fruits et légumes régulièrement pourrait réduire votre risque de problèmes cardiaques d'environ 23%."},
        {"llm_text": "Il est recommandé d'effectuer une sauvegarde des données avant de procéder à la mise à jour du système d'exploitation.",
         "human_text": "Je vous conseille vraiment de faire une sauvegarde de vos données avant de mettre à jour votre système."},
        {"llm_text": "En conclusion, les données ne permettent pas de rejeter l'hypothèse nulle avec un niveau de signification de 0.05.",
         "human_text": "En fin de compte, nos résultats ne sont pas assez concluants pour rejeter l'hypothèse de départ (avec un seuil de 0.05)."},
        {"llm_text": "La température optimale de fonctionnement se situe entre 20 et 25 degrés Celsius.",
         "human_text": "L'appareil fonctionne mieux quand il fait entre 20 et 25 degrés dans la pièce."},
        {"llm_text": "L'analyse des commentaires révèle un sentiment généralement positif avec 68% d'opinions favorables.",
         "human_text": "En lisant les commentaires, on voit que la plupart des gens sont contents - environ deux tiers ont laissé des avis positifs!"},
        {"llm_text": "La méthode proposée présente une amélioration de l'efficacité de 15% par rapport aux approches conventionnelles.",
         "human_text": "Notre nouvelle méthode est 15% plus efficace que les techniques habituelles."},
        {"llm_text": "Les utilisateurs doivent s'authentifier via le portail sécurisé avant d'accéder aux ressources protégées.",
         "human_text": "Vous devez vous connecter sur le portail sécurisé avant de pouvoir accéder à ces ressources."},
        {"llm_text": "La durée estimée pour compléter cette tâche est approximativement de 3 heures et 45 minutes.",
         "human_text": "Ce travail devrait vous prendre environ 3h45 à terminer."},
        # Nouveaux exemples pour enrichir le dataset
        {"llm_text": "Veuillez noter que l'événement a été reprogrammé à une date ultérieure en raison de circonstances imprévues.",
         "human_text": "On a dû décaler l'événement à plus tard à cause d'un imprévu, désolé!"},
        {"llm_text": "Il est impératif de procéder à une vérification minutieuse des documents avant leur soumission définitive.",
         "human_text": "Il faut bien vérifier vos documents avant de les soumettre définitivement."},
        {"llm_text": "Les participants sont priés de confirmer leur présence au minimum 48 heures avant le début de la conférence.",
         "human_text": "Merci de nous dire si vous venez au moins 2 jours avant la conférence!"},
        {"llm_text": "Le rapport financier trimestriel indique une augmentation des revenus de 7.2% par rapport à la période précédente.",
         "human_text": "On a vu nos revenus grimper de 7.2% ce trimestre par rapport au précédent!"},
        {"llm_text": "Suite à l'analyse des données recueillies, il apparaît que la stratégie A est significativement plus performante que la stratégie B.",
         "human_text": "Après avoir analysé toutes les données, on voit clairement que la stratégie A marche bien mieux que la B."}
    ]

    # Nombre de répétitions nécessaires pour atteindre la taille demandée
    repetitions = max(1, size // len(data) + 1)

    # Augmenter les données pour avoir un dataset plus grand
    extended_data = []
    for _ in range(repetitions):
        extended_data.extend(data)

    # Limiter à la taille demandée
    extended_data = extended_data[:size]

    # Sauvegarder avec gestion des erreurs
    try:
        df = pd.DataFrame(extended_data)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        df.to_csv(output_path, index=False)
        print(f" Dataset d'exemple créé: {output_path} ({len(df)} exemples)")
        return df
    except Exception as e:
        print(f" Erreur lors de la création du dataset: {str(e)}")
        return pd.DataFrame(data)

# === 6. Fonction d'entraînement optimisée ===
def train_model(
    model_name: str = "t5-small",
    data_path: str = "data/humanized_pairs.csv",
    output_dir: str = "model/saved_model",
    num_epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 5e-5,
    device: Optional[str] = None,
    use_styles: bool = False,
    checkpoint_dir: str = "model/checkpoints",
    max_length: int = 256,
    auto_save: bool = True,
    save_interval: int = 1,
    show_metrics: bool = True,
):
    start_time = time.time()

    if not device:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    print(f" Entraînement sur {device}")
    print(f"   Modèle: {model_name}")
    print(f"   Données: {data_path}")
    print(f"   Époque: {num_epochs}")
    print(f"   Batch size: {batch_size}")

    try:
        # Charger modèle et tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        if device == "cuda":
            model = model.half()
        model.to(device)

        # Charger données
        if data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        elif data_path.endswith('.jsonl') or data_path.endswith('.json'):
            df = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))
        else:
            raise ValueError("Format de fichier non pris en charge. Utilisez CSV ou JSONL.")

        required_columns = ['llm_text', 'human_text']
        if not all(col in df.columns for col in required_columns):
            alternatives = {
                'llm_text': ['machine_text', 'bot_text', 'ai_text', 'generated_text'],
                'human_text': ['humanized_text', 'natural_text', 'target_text']
            }
            for req_col, alt_cols in alternatives.items():
                if req_col not in df.columns:
                    for alt_col in alt_cols:
                        if alt_col in df.columns:
                            df[req_col] = df[alt_col]
                            break

        if not all(col in df.columns for col in required_columns):
            raise ValueError(f"Le dataset doit contenir les colonnes {required_columns}")

        llm_texts = df['llm_text'].tolist()
        human_texts = df['human_text'].tolist()

        print(f" Données chargées: {len(llm_texts)} paires")

        if use_styles:
            styles = ["formel", "amical", "enthousiaste"]
            styled_llm_texts = []
            styled_human_texts = []

            print("Génération de variations de style...")
            for i, (llm_text, human_text) in enumerate(zip(llm_texts, human_texts)):
                styled_llm_texts.append(llm_text)
                styled_human_texts.append(human_text)
                if i % 2 == 0:
                    style = styles[i % len(styles)]
                    styled_llm_texts.append(f"[{style}] {llm_text}")
                    styled_human_texts.append(human_text)

            llm_texts, human_texts = styled_llm_texts, styled_human_texts
            print(f" Dataset enrichi avec styles: {len(llm_texts)} exemples")

        train_texts, val_texts, train_human, val_human = train_test_split(
            llm_texts, human_texts, test_size=0.2, random_state=42
        )

        train_dataset = HumanizerDataset(train_texts, train_human, tokenizer, max_length=max_length)
        val_dataset = HumanizerDataset(val_texts, val_human, tokenizer, max_length=max_length)

        training_args = Seq2SeqTrainingArguments(
            output_dir=checkpoint_dir,
            eval_steps=100,
            save_steps=100,
            learning_rate=learning_rate,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=2,
            num_train_epochs=num_epochs,
            predict_with_generate=True,
            fp16=device == "cuda",
            logging_dir=os.path.join(output_dir, "logs"),
            logging_steps=10,
            report_to="none",

        )

        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding="max_length",
            max_length=max_length
        )

        def compute_metrics(eval_preds):
            preds, labels = eval_preds
            predictions = tokenizer.batch_decode(preds, skip_special_tokens=True)
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            references = tokenizer.batch_decode(labels, skip_special_tokens=True)
            metrics = calculate_simple_metrics(predictions, references)
            return metrics

        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics
        )

        # --- AJOUTER METRICS CALLBACK ---
        class MetricsCallback(transformers.TrainerCallback):
            def __init__(self, trainer, eval_dataset, tokenizer, output_dir, save_interval):
                self.trainer = trainer
                self.eval_dataset = eval_dataset
                self.tokenizer = tokenizer
                self.output_dir = output_dir
                self.save_interval = save_interval
                self.best_metric = float('inf')
                self.step_metrics = {}

            def on_evaluate(self, args, state, control, metrics, **kwargs):
                self.step_metrics[state.global_step] = metrics

                if show_metrics:
                    print(f"\n MÉTRIQUES À L'ÉTAPE {state.global_step}")
                    print("-" * 50)
                    for key, value in metrics.items():
                        if isinstance(value, float):
                            print(f"{key}: {value:.4f}")
                        else:
                            print(f"{key}: {value}")

                if auto_save and (state.global_step % (100 * save_interval) == 0):
                    test_text = "L'analyse des données indique une corrélation significative entre les variables étudiées."
                    temp_dir = os.path.join(output_dir, "temp_checkpoint")
                    os.makedirs(temp_dir, exist_ok=True)
                    model.save_pretrained(temp_dir)
                    tokenizer.save_pretrained(temp_dir)
                    try:
                        test_model = TextHumanizerModel.load_model(temp_dir, device)
                        humanized = get_humanized_text(test_model, test_text)
                        checkpoint_dir = os.path.join(output_dir, f"checkpoint-{state.global_step}")
                        os.makedirs(checkpoint_dir, exist_ok=True)
                        model.save_pretrained(checkpoint_dir)
                        tokenizer.save_pretrained(checkpoint_dir)
                        with open(os.path.join(checkpoint_dir, "model_info.json"), "w") as f:
                            json.dump({
                                "step": state.global_step,
                                "metrics": metrics,
                                "test_output": humanized,
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            }, f, indent=2)
                        print(f" Modèle testé et sauvegardé à l'étape {state.global_step}")
                        print(f"   Exemple de sortie: \"{humanized}\"")
                    except Exception as e:
                        print(f" Erreur lors du test du modèle: {str(e)}")

        trainer.add_callback(MetricsCallback(trainer, val_dataset, tokenizer, output_dir, save_interval))

        # --- Démarrer l'entraînement ---
        try:
            trainer.train()

            # --- Évaluation finale ---
            print("\n ÉVALUATION FINALE DU MODÈLE")
            test_df = pd.DataFrame({
                'llm_text': val_texts[:10],
                'human_text': val_human[:10]
            })
            humanizer = TextHumanizerModel.load_model(output_dir, device)
            eval_results = evaluate_model(humanizer, test_df)

            with open(os.path.join(output_dir, "evaluation_results.json"), "w") as f:
                serializable_results = {
                    "metrics": eval_results["metrics"],
                    "accuracy": eval_results["accuracy"],
                    "examples": [
                        {
                            "original": val_texts[i],
                            "reference": val_human[i],
                            "prediction": eval_results["predictions"][i]
                        }
                        for i in range(min(5, len(val_texts)))
                    ]
                }
                json.dump(serializable_results, f, indent=2)
            print(f"Résultats d'évaluation sauvegardés dans {output_dir}/evaluation_results.json")

        except KeyboardInterrupt:
            print("Entraînement interrompu manuellement.")

        # Sauvegarde finale
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)

        metadata = {
            "model_name": model_name,
            "base_model": model_name,
            "dataset_size": len(llm_texts),
            "epochs": num_epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "max_length": max_length,
            "training_time": f"{(time.time() - start_time) / 60:.2f} minutes",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(os.path.join(output_dir, "model_info.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        print(f" Entraînement terminé en {(time.time() - start_time) / 60:.2f} minutes")
        print(f" Modèle sauvegardé dans {output_dir}")

        return {"status": "success", "output_dir": output_dir, "training_time": time.time() - start_time}

    except Exception as e:
        print(f" Erreur lors de l'entraînement: {str(e)}")
        return {"status": "error", "error": str(e)}
# Ajouter cette fonction pour tester facilement le modèle
def test_model_accuracy(model_path="model/saved_model", custom_test=None):
    """
    Teste un modèle entraîné et affiche son exactitude et autres métriques

    Args:
        model_path: Chemin vers le modèle entraîné
        custom_test: Texte personnalisé à tester
    """
    try:
        # Charger le modèle
        print(f"Chargement du modèle depuis {model_path}...")
        model = TextHumanizerModel.load_model(model_path)

        # Évaluer sur des exemples standard
        eval_results = evaluate_model(model)

        if custom_test:
            print("\n TEST PERSONNALISÉ")
            print("-" * 50)
            print(f"Original: {custom_test}")
            humanized = get_humanized_text(model, custom_test)
            print(f"Humanisé: {humanized}")

        # Interface interactive simple
        print("\nTESTER D'AUTRES EXEMPLES")
        print("-" * 50)
        print("Entrez du texte à humaniser (ou 'q' pour quitter):")

        while True:
            user_input = input("\nTexte: ")
            if user_input.lower() == 'q':
                break

            if user_input:
                try:
                    humanized = get_humanized_text(model, user_input)
                    print(f"\nRésultat: {humanized}")
                except Exception as e:
                    print(f" Erreur: {str(e)}")
            else:
                print("Texte vide. Veuillez entrer un texte.")

        return eval_results

    except Exception as e:
        print(f" Erreur lors du test du modèle: {str(e)}")
        return None

# === 7. API REST ===
app = FastAPI(title="API d'Humanisation de Texte LLM")

class TextRequest(BaseModel):
    text: str
    style: Optional[str] = ""

class TextResponse(BaseModel):
    original: str
    humanized: str
    processing_time: float

# Variable globale pour stocker le modèle
global_model = None
model_loading = False
model_lock = threading.Lock()

def load_model_bg(model_path: str, device: Optional[str] = None):
    """Charge le modèle en arrière-plan"""
    global global_model, model_loading

    try:
        print(f"Chargement du modèle depuis {model_path}...")
        model = TextHumanizerModel.load_model(model_path, device)

        with model_lock:
            global_model = model
            model_loading = False

        print(" Modèle chargé avec succès")
    except Exception as e:
        print(f" Erreur lors du chargement du modèle: {str(e)}")
        model_loading = False

@app.post("/humanize", response_model=TextResponse)
async def humanize_text(request: TextRequest, background_tasks: BackgroundTasks):
    """Endpoint pour humaniser du texte"""
    global global_model, model_loading

    # Vérifier si le modèle est chargé
    if global_model is None:
        if not model_loading:
            model_loading = True
            # Charger le modèle en arrière-plan
            background_tasks.add_task(load_model_bg, "model/saved_model")

        raise HTTPException(
            status_code=503,
            detail="Le modèle est en cours de chargement. Veuillez réessayer dans quelques instants."
        )

    # Humaniser le texte
    start_time = time.time()
    try:
        humanized = get_humanized_text(global_model, request.text, request.style)
        processing_time = time.time() - start_time

        return {
            "original": request.text,
            "humanized": humanized,
            "processing_time": processing_time
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de l'humanisation: {str(e)}")

# === 8. Interface utilisateur dans Colab ===
def create_ui(model_path="model/saved_model"):
    """Crée une interface utilisateur simple dans Colab"""
    global global_model

    # Charger le modèle s'il n'est pas déjà chargé
    if global_model is None:
        try:
            global_model = TextHumanizerModel.load_model(model_path)
        except Exception as e:
            print(f" Erreur lors du chargement du modèle: {str(e)}")
            print("Continuer avec un modèle par défaut...")
            global_model = TextHumanizerModel()

    # Créer les widgets
    style_dropdown = widgets.Dropdown(
        options=[
            ('Standard', ''),
            ('Formel', '[formel]'),
            ('Amical', '[amical]'),
            ('Enthousiaste', '[enthousiaste]')
        ],
        description='Style:',
        disabled=False,
    )

    text_area = widgets.Textarea(
        value='Exemple: La procédure d\'installation requiert les étapes suivantes: téléchargement du fichier, extraction des données et configuration des paramètres initiaux.',
        placeholder='Entrez le texte à humaniser',
        description='Texte:',
        disabled=False,
        layout=widgets.Layout(width='100%', height='100px')
    )

    output_area = widgets.Output(layout=widgets.Layout(border='1px solid black', padding='10px', width='100%'))

    humanize_button = widgets.Button(
        description='Humaniser',
        disabled=False,
        button_style='primary',
        tooltip='Cliquez pour humaniser le texte',
        icon='magic'
    )

    # Fonction pour humaniser le texte
    def on_button_click(b):
        with output_area:
            clear_output()
            print(" Humanisation en cours...")

            try:
                start_time = time.time()
                humanized = get_humanized_text(global_model, text_area.value, style_dropdown.value)
                elapsed = time.time() - start_time

                clear_output()
                print(" Texte humanisé:")
                print(f"\n Original: \"{text_area.value}\"")
                print(f"\n Humanisé: \"{humanized}\"")
                print(f"\n Temps de traitement: {elapsed:.2f} secondes")
            except Exception as e:
                clear_output()
                print(f" Erreur: {str(e)}")

    # Associer la fonction au bouton
    humanize_button.on_click(on_button_click)

    # Afficher l'interface
    display(HTML("<h2>Humaniseur de Texte LLM</h2>"))
    display(text_area)
    display(style_dropdown)
    display(humanize_button)
    display(output_area)

# === 9. Fonction principale pour lancer le service ===
def start_api_server():
    """Démarre le serveur API"""
    try:
        # Configurer le tunnel ngrok
        from pyngrok import ngrok

        # Démarrer le serveur uvicorn dans un thread séparé
        port = 8000
        thread = threading.Thread(target=lambda: uvicorn.run(app, host="127.0.0.1", port=port))
        thread.daemon = True
        thread.start()

        # Créer un tunnel ngrok
        public_url = ngrok.connect(port).public_url
        print(f" API accessible à: {public_url}")
        print("Documentation API: {}/docs".format(public_url))

        # Charger le modèle en arrière-plan
        global model_loading
        model_loading = True
        threading.Thread(target=lambda: load_model_bg("model/saved_model")).start()

    except Exception as e:
        print(f" Erreur lors du démarrage du serveur: {str(e)}")

# === 10. Fonction principale ===
def run_demo():
    """
    Exécute la démonstration complète:
    1. Vérifie le GPU
    2. Crée la structure de répertoires
    3. Crée un dataset d'exemple
    4. Entraîne le modèle
    5. Lance l'interface utilisateur
    """
    print(" Démarrage de la démonstration d'humanisation de texte LLM")

    # Vérifier le GPU
    has_gpu = check_gpu()

    # Créer la structure de répertoires
    setup_directories()

    # Créer un dataset d'exemple
    df = create_sample_dataset(size=100)

    # Demander à l'utilisateur s'il souhaite entraîner le modèle
    train_widget = widgets.Button(
        description='Entraîner le modèle',
        button_style='primary',
        tooltip='Cliquez pour entraîner le modèle'
    )

    demo_widget = widgets.Button(
        description='Utiliser le modèle pré-entraîné',
        button_style='info',
        tooltip='Cliquez pour utiliser le modèle pré-entraîné'
    )

    output_widget = widgets.Output()

    def on_train_click(b):
        with output_widget:
            clear_output()
            print(" Entraînement du modèle...")

            # Adapter les paramètres selon la disponibilité du GPU
            batch_size = 8 if has_gpu else 4
            model_name = "t5-small"  # Modèle plus petit pour Colab

            result = train_model(
                model_name=model_name,
                data_path="data/humanized_pairs.csv",
                num_epochs=3,
                batch_size=batch_size,
                learning_rate=5e-5,
                use_styles=True,
                max_length=256  # Réduire pour économiser la mémoire
            )

            if result["status"] == "success":
                print("\n Entraînement terminé")
                print("Lancement de l'interface utilisateur...")
                create_ui("model/saved_model")
            else:
                print(f"\n Erreur lors de l'entraînement: {result.get('error', 'Erreur inconnue')}")

    def on_demo_click(b):
        with output_widget:
            clear_output()
            print(" Chargement du modèle pré-entraîné...")

            # Utiliser le modèle de base pour la démonstration
            global global_model
            global_model = TextHumanizerModel()

            print("\n Modèle prêt")
            print("Lancement de l'interface utilisateur...")
            create_ui()

    # Associer les fonctions aux boutons
    train_widget.on_click(on_train_click)
    demo_widget.on_click(on_demo_click)

    # Afficher les boutons
    display(HTML("<h2>Humaniseur de Texte LLM - Démarrage</h2>"))
    display(HTML("<p>Choisissez une option pour continuer:</p>"))
    display(widgets.HBox([train_widget, demo_widget]))
    display(output_widget)

# === 11. Point d'entrée principal ===
if __name__ == "__main__":
    # Montage de Google Drive (optionnel)
    mount_drive = False
    if mount_drive:
        try:
            drive.mount('/content/drive')
            print(" Google Drive monté avec succès")
        except Exception as e:
            print(f" Erreur lors du montage de Google Drive: {str(e)}")

    # Exécuter la démonstration
    run_demo()

create_ui("model/saved_model")
