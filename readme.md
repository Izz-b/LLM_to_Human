#### Construire un modèle capable de reformuler la sortie d'un LLM pour la rendre plus "humaine"

### Text Humanizer: Documentation

#### Introduction
**Text Humanizer** is an AI tool designed to transform texts generated by large language models (LLMs) into more natural and human-like language. This project implements a sequence-to-sequence (seq2seq) approach based on the T5 architecture, optimized to run efficiently in a Google Colab environment.

#### Table of Contents
- Overview of the Project
- Installation and Setup
- Model Architecture
- Usage
- User Interface
- REST API
- Model Training
- Performance Evaluation
- Optimizations
- Transformation Examples
- Limitations and Future Work
- Function Reference

#### Overview of the Project
The **Text Humanizer** project addresses the growing need to make AI-generated text sound more natural and less detectable as machine-produced. Through supervised learning, the model learns to transform typically "LLM" phrases (formal, structured, sometimes rigid) into sentences that reflect the nuances and fluidity of human language.

**Key Features:**
- Transformation of LLM text into human-like style
- Support for multiple transformation styles (formal, friendly, enthusiastic)
- Intuitive user interface in Google Colab
- REST API for integration with other services
- Optimizations for effective performance with limited resources

#### Installation and Setup

**Prerequisites:**
- Access to Google Colab (preferably with GPU)
- Python 3.7+

**Installing Dependencies:**
```bash
!pip install -q fastapi uvicorn transformers datasets tqdm ipywidgets pyngrok
```

**Directory Structure:**
```
text_humanizer/
├── model/
│   ├── saved_model/  # Trained model
│   └── checkpoints/  # Training checkpoints
├── data/             # Training data
└── exports/          # Model exports
```

#### Model Architecture

**Technology Stack:**
Text Humanizer is built on the T5 architecture (Text-to-Text Transfer Transformer), specifically the "t5-small" variant, which offers a good balance between performance and resource demands.

**Learning Approach:**
The model uses a sequence-to-sequence (seq2seq) approach, where:
- **Input:** LLM-generated text, prefixed with "humanize:"
- **Output:** The humanized version of the same text

**Transformation Styles:**
- **Standard:** General transformation
- **Formal:** Neat and professional style
- **Friendly:** Conversational and relaxed tone
- **Enthusiastic:** Energetic and expressive style

#### Usage

**User Interface:**
The interface in Google Colab offers a simple and direct usage experience:
- Run `run_demo()` to start the interface
- Choose to either train a new model or use a pre-trained model
- In the main interface:
  - Enter the text to humanize in the text box
  - Select a style from the dropdown
  - Click "Humanize" to transform the text
  - View the result and processing time

**REST API:**
The project includes a fully functional REST API for integration into other services.

**Starting the Server:**
```bash
start_api_server()
```
This command:
- Starts a local **uvicorn** server on port 8000
- Creates an **ngrok** tunnel to make the API accessible externally
- Displays the public URL and link to the documentation

**Available Endpoints:**
- **POST /humanize**  
  Request body: `{"text": "Text to humanize", "style": "[optional_style]"}`
  Response: `{"original": "Original text", "humanized": "Humanized text", "processing_time": time_in_seconds}`

Interactive Swagger documentation is available at `{public_url}/docs`.

#### Model Training

**Preparing Data:**
The model is trained on pairs of texts (LLM text → human text). The project includes a function `create_sample_dataset()` that generates an example dataset. You can also use your own dataset in CSV or JSONL format with the following columns:
- `llm_text`: LLM-generated text
- `human_text`: Humanized version of the same text

**Training the Model:**
To train the model:
```python
result = train_model(
    model_name="t5-small",
    data_path="data/humanized_pairs.csv",
    num_epochs=3,
    batch_size=8,  # Reduce if memory is limited
    learning_rate=5e-5,
    use_styles=True,
    max_length=256
)
```

**Training Parameters:**
- `model_name`: Base model to use (default: "t5-small")
- `data_path`: Path to the dataset file
- `output_dir`: Output directory for the trained model
- `num_epochs`: Number of training epochs
- `batch_size`: Batch size (reduce if memory is limited)
- `learning_rate`: Learning rate
- `device`: Device to train on ("cpu" or "cuda")
- `use_styles`: Enable style-enhanced data
- `max_length`: Maximum sequence length

#### Performance Evaluation

The model evaluates its performance using several metrics:
- **Length Difference:** Measures the relative length difference between the original and transformed text.
- **Word Count Ratio:** Compares the word count between the original and transformed text.
- **Lexical Similarity:** Evaluates the proportion of common words between the two texts.

These metrics are automatically calculated during training and evaluation.

#### Optimizations

The project includes several optimizations to ensure efficient performance in Google Colab:
- **GPU Optimizations**
- **Mixed Precision (FP16) on GPU**
- **Model Conversion to Half-Precision for Memory Savings**
- **Batch Size and Sequence Length Adjustments**
- **LRU Cache for Frequent Requests**
- **Background Model Loading**
- **Error Handling and Interrupt Management**
- **Periodic Checkpoint Saving**

#### Transformation Examples

**LLM Original Text** | **Humanized Text**  
--- | ---  
"Les résultats de l'analyse indiquent une corrélation positive entre les variables A et B avec un coefficient de 0.78." | "Notre analyse montre que A et B sont fortement liés - on a trouvé un coefficient de 0.78!"  
"La procédure d'installation requiert les étapes suivantes: téléchargement du fichier, extraction des données et configuration des paramètres initiaux." | "Pour installer le programme, vous devez d'abord télécharger le fichier, puis extraire les données et enfin configurer vos paramètres de départ."  
"Il est recommandé d'effectuer une sauvegarde des données avant de procéder à la mise à jour du système d'exploitation." | "Je vous conseille vraiment de faire une sauvegarde de vos données avant de mettre à jour votre système."

#### Limitations and Future Work

**Current Limitations:**
- The model is limited by the size and diversity of the training dataset.
- The transformation might sometimes slightly alter the meaning or omit details.
- Performance is constrained by the limited resources of Google Colab.

**Possible Improvements:**
- Enrich the dataset with more examples from various domains.
- Explore more advanced architectures (T5-base, T5-large).
- Add additional transformation styles (technical, educational, etc.).
- Implement automatic style detection for the source text.
- Improve evaluation metrics with BERT-based scores.

#### Function Reference

**Main Functions:**
- `TextHumanizerModel`: The main class for text humanization
- `generate()`: Transforms LLM text into humanized text
- `save_model()`: Saves the trained model
- `load_model()`: Loads a saved model
- `train_model()`: Trains the model on a dataset
- `get_humanized_text()`: Humanizes text with optional style
- `create_sample_dataset()`: Generates an example dataset
- `calculate_simple_metrics()`: Calculates evaluation metrics

**Interface Functions:**
- `create_ui()`: Creates the user interface in Colab
- `start_api_server()`: Starts the API server with ngrok tunnel
- `run_demo()`: Launches the full demo

**Utility Functions:**
- `check_gpu()`: Checks GPU availability
- `setup_directories()`: Creates necessary directory structure
- `simple_tokenize()`: Tokenizes text for evaluation

